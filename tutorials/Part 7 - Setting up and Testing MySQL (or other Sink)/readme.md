# Using a Sink Job

So an EDNA application needs a Source Job, a Sink Job, and has some intermediate Jobs. We have already covered a couple types of Sources. This part covers setting up a sink and connecting to it.

I will do this with mysql, since I have written an Emit primitive for it in `edna.emit.SQLEmit`. The core idea would work with any type of sink, however, such as an S3 bucket, mongo db, cassandra db, Azure blob, etc.

## Setting up the sink
You can follow the steps [here to get mysql set up](https://dev.mysql.com/downloads/installer/) for Windows. You can follow steps [here for Ubuntu](https://www.digitalocean.com/community/tutorials/how-to-install-mysql-on-ubuntu-16-04) and [these for macOS](https://www.positronx.io/how-to-install-mysql-on-mac-configure-mysql-in-terminal/). I can't really test the mac installer, so if you know of better ways, feel free to use them. Install it outside the cluster, since we want the cluster to only run Applications and Jobs.

Once it is installed and the mysql service has started, start a mysql session, so that you get something like the following on Ubuntu; steps should be similar for windows on cmd/powershell and mac on Terminal (See just after this section for an important note for Windows and WSL2)

```
[asuprem@Suprem-Laptop: ~] $ sudo mysql -u root -p
Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 7
Server version: 5.7.31-0ubuntu0.16.04.1 (Ubuntu)

Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql>
```

At this point, you are likely logged in as the root user. If you already set up a non-root user, log in using that to make sure it works (and make sure it has a password). If you haven't set up a non-root user, do the following:

```
CREATE USER '<username>'@'localhost' IDENTIFIED BY '<password>';
```

So, an (insecure) example:

```
CREATE USER 'asuprem'@'localhost' IDENTIFIED BY '1234';
```

Then grant this user priviledges to create and read tables with:

```
GRANT ALL PRIVILEGES ON * . * TO '<username>'@'localhost';
```

This is obviously not a good idea in terms of security in a production environment, since this is basically a root account without the frills, but because we are using this at a small scale, we'll let security lapses be.

Exit out and log back in with your newly created user account to test it:

```
sudo mysql -u <username> -p
```

## An important note for Windows and WSL2
You have installed the mysql server on Windows. Since it is a server, it can technically be accessed from anywhere on your computer as long as you have access to localhost.

But WSL2 is still having it's kinks worked out, and one of them is a non-functional network bridge between the host (Windows) and the VM (WSL2). Essentially, if you host something on the WSL2 localhost, you can access it on Windows. For example, the docker `joxit` image from a few parts ago that was hosted on the WSL2 vm's `localhost:80` could be accessed from your Windows browser. However, something hosted on Windows localhost (i.e. the mysql server, which is hosted on `localhost:3306`) can't be easily accessed from WSL2.

What you need to do us use the built-in port forwarding and manually find the Windows host IP. This IP is in `/etc/resolv.conf`, which you can find with:


```
[asuprem@Suprem-Laptop: .../c/Users/asupr] $ cat /etc/resolv.conf
# This file was automatically generated by WSL. To stop automatic generation of this file, add the following entry to /etc/wsl.conf:
# [network]
# generateResolvConf = false
nameserver 172.28.144.1
```

The host IP is (in my case) `172.28.144.1`. This will likely be different for everyone, and it will change each time you restart the VM. An important note regarding VM restart -- just closing the WSL2 bash window does not end the VM, fortunately. Restarting your laptop or manually terminating the VM with `wsl --terminate <wsl2 distro name>` in powershell are the few ways to terminate the session.

In any case, from your WSL2 vm, you can access mysql with the nameserver IP from `resolv.conf`. This is important for kubernetes, because it needs to find a way to access this API, which we will set up later.

For now, you can try to verify this by installing the mysql client on your WSL2 vm with `sudo apt-get install mysql-client` and running:

```
mysql -h <nameserver IP> -u <username> -p
```

This should connect to the mysql installation.

## Sanity checks (for everyone)
During your mysql install, you might have been prompted to install an example database. You can check for this by logging into your mysql server from a client and checking what databases exist with:

```
mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sakila             |  <----- THIS IS THE SAMPLE (i.e. sakila)
| sys                |
| world              |
+--------------------+
6 rows in set (0.30 sec)
```
If you don't, no worries -- we'll create a dummy in the next section. If you do, select it with:

```
mysql> use sakila;
```

and check what tables exist with:

```
mysql> show tables;
```

One of them should be `actor`. If there isn't an actor table, also go to the next section, where we will create a dummy database and table, and skip to the part about creating the table.

## Sanity check part 2 -- creating a dummy database and table for the samples

Create the database with:

```
mysql> CREATE DATABASE sakila;
```

Then switch to it with:

```
mysql> use sakila;
```

Then create the actor table with:

```
CREATE TABLE actor (
  actor_id smallint unsigned NOT NULL AUTO_INCREMENT,
  first_name varchar(45) NOT NULL,
  last_name varchar(45) NOT NULL,
  last_update timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (actor_id),
  KEY idx_actor_last_name (last_name)
)
```


## Some useful protocols for data access
In a production-type environment, there obviously needs to be more security in data access. We won't deal with that here, but example policiies can include:

1. A dedicated user for each application
2. Granting an application read access only to tables it needs to read
3. Granting an application write access only to tables it needs to write
4. Separation of production and development databases
5. Data replication for backup and recovery

You can, of course, implement some of these; for example it would be trivial to create a new user for your test application.

# A Sink Example
Now that we have a sink, we will test it with a Sink Job. We will also test a couple Processes.

I have added a `SQLEmitSample` folder in `examples/job-examples`. Let's go through it first.

## Overview of the Job
The `SQLEmitSample` takes in a stream of actors and saves them to the `actor` table in the `sakila` database. We will follow the basic *ingest-process-emit* loop we have established, with the following:

- **Ingest**: We will use a `SimulatedIngest` to simulate a stream. For a simulated ingest, we essentially provide a list of elements that the primitive will emit one by one.
- **Process**: Our ingest will be in the form of json strings. We will need to convert the json string to a type we can save to SQL. This is because a SQL statement through the sql connector expects two tuples: one of the fields (i.e. table columns) and one of the values for those fields. We will use two operations: one process to deserialize the json string to an object (i.e. a dictionary) and another process to convert the object into a SQL schema format (discussed later).
- **Emit**: The emit process will insert into the SQL table.

## Code overview

First, we create the stream we want to simulate:

```
list_of_inserts = ['{"first_name":"jessica", "last_name":"st. german", "additional":"unneeded1"}',
            '{"first_name":"jessica", "last_name":"courtney", "additional":"unneeded2"}', 
            '{"first_name":"jessica", "last_name":"mishra", "additional":"unneeded3"}', 
            '{"first_name":"jessica", "last_name":"novinha", "additional":"unneeded4"}',
            '{"first_name":"jessica", "last_name":"tudor", "additional":"unneeded5"}',
            '{"first_name":"jessica", "last_name":"ael-rayya", "additional":"unneeded6"}', 
            '{"first_name":"jessica", "last_name":"zuma", "additional":"unneeded7"}', 
            '{"first_name":"jessica", "last_name":"akihita", "additional":"unneeded8"}',
            '{"first_name":"jessica", "last_name":"xi", "additional":"unneeded9"}',
            '{"first_name":"jessica", "last_name":"kurylova", "additional":"unneeded10"}']
```

Here, each entry has a first name, a last name, and an `additional` column that we won't insert. This simulates a stream object that may only have some important fields.

Next we set up the streaming context.

```
context = StreamingContext()
```

Then we set up a tuple factory with:

```
tuple_factory = SQLTupleFactory(tuple_fields=context.getVariable("sql_fields")) 
```

A factory in OOP is a class or method that constructs and returns on object. The SQLTupleFactory does something like this. It's `getValues()` method will, given a dictionary and a schema, extract the relevant values from the dictionary. Here, we want only the first name and last name from our stream. So we will set the schema with the `tuple_fields` argument, and pass it from the context. In `ednaconf.yaml`, we set the `first_name` and `last_name` as the schema we want.

Once the `tuple_factory` is initialized, it can be called with `tuple_factory.getValues(some_object)`. If `some_object` is the first element from our simulated stream, `tuple_factory` will return `("jessica","st. german")` (leaving out the uneeded `additional` column). The `getFields()` method returns the schema itself: `("first_name", "last_name")`. This also preserves ordering.

We then set up our Ingest with `SimulatedIngest`, with an `EmptySerializer` since we don't need to process the string. Technically, we could create a `JsonSerializer` that will deserialize the string, but it's a good idea not to have any additional processing in an Ingest beyond what is absolutely necessary; for example, if our imputs were in bytes, then we would need a `StringSerializer` to convert byte back to string. But in this case, we already have a stream of Strings that we can operate on. We also pass in the simulated stream with `stream_list`.

```
SimulatedIngest(stream_list=list_of_inserts)
```

Next, we set up our processes. As a reminder, first we need to convert the json string to an object. Then we need to convert the object to a tuple format. Each of these is what's called a **map** transformation: in a map transformation, each element in a stream is converted to another element (compared to something like a **flatmap** operation, where an element in a stream can be converted to more than 1 element).

We can chain **map** processes trivially since each of them is a 1-1 operation. 

```
StreamBuilder.build(ingest).map(JsonToObject()).map(ObjectToSQL(tuple_factory=tuple_factory))
```

Here, we applied two **map** processes: first a `JsonToObject()` to convert the incoming json string to a dictionary, and then an `ObjectoSQL()` with the created `tuple_factory`.

Side note: `ObjectToSQL` is basically a weapper around  `SQLTupleFactory`; we make `SQLTupleFactory` its own class because for a large job, an instance of the factory could be used in multiple places and making it its own object can allow for easier resource management down the line.

Then, we create the Emit with `SQLEmit`:

```
SQLEmit( 
      database=context.getVariable("database"), 
      host=context.getVariable("host"), 
      user=context.getVariable("user"), 
      password=context.getVariable("password"),
      table=context.getVariable("table"),
      tuple_factory=tuple_factory)
```

Note that we pass the same instance of `SQLTupleFactory` (i.e. `tuple_factory`) here. If you look inside `SQLEmit`, you will see the following:

```
def build_statement(self):
    if self.query_base is None:
        fields = ",".join(self.tuple_factory.getFields())
        value_types = ",".join(["%s"]*self.tuple_factory.getFieldCount())
        self.query_base = "INSERT INTO {table} ({fields}) VALUES ({value_types}) ".format(table=self.table, fields = fields, value_types = value_types)
```

This creates a SQL query, and uses the `tuple_factory`'s `getFields()` method to get the fields for the SQL statement.

Then, in the `write()` method, we have

```
cursor.executemany(self.query_base, self.emit_buffer[:self.emit_buffer_index+1])
```

where the `emit_buffer` contains the actual values obtained by the previous process `ObjectToSQL`'s `map()` method and stored in the buffer, with:

```
# For ObjectToSQL
def map(self, record: Dict):
    return self.tuple_factory.getValues(record=record)    
```

## Some additional notes
One thing I want to draw attention to is the `emit_buffer` in `BaseEmit`. Be sure to take a look at it. Basically, `BaseEmit` (which is the parent of all emit primitives) will store all the records it receives in a buffer and emit several of them at once, instead of emitting one at a time. This is because emitting several records at a time is almost always more efficient in any context: writing to a database, writing to disk, sending to a web service, etc.

`BaseEmit`, in its initialization, has two important arguments for this: `emit_buffer_batch_size` and `emit_buffer_timeout_ms`. This control how long a buffer is stored before its contents are emitted. The logic is in the `__call()__` method. First checks if it has been too long since the last emit -- this is the buffer timeout. The default value is 100ms, i.e. there should be, at minimum, 10 emits per second. This can be increased to trade higher latency for lower overhead in emitting (because each emit has an overhead). Within timeouts, `BaseEmit` also checks if the buffer is full with the batch size -- which is default to 10. So if the timeout has not been reached but there are 10 items in the buffer already, `BaseEmit` will emit those records, empty the buffer, and reset the timeout. 

Since I am still in the midst of implementing a network stack that will asynchronously handle transfer between primitives, right now the whole Job operates sequentially. So if you have a SimulatedIngest, you should have multiples of the `emit_buffer_batch_size` in the SimulatedIngest, so that you can correctly test your Job. In this case, you can either overrile the default 10 by passing in a different value to `SQLEmit` (which will, in turn, pass these values to `BaseEmit`) or you can just have multiples of 10 in the simulated ingest -- the example has, e.g., 10. If you have 15, the last 5 will never emit because the Ingest will basically block the rest of the Job from progressing. This is obviously a limitation that I would address soon, but for now, an important thing to keep in mind. A naive way to bypass this is to set the `emit_buffer_batch_size` to 1 that the emitter processes records one by one (this would also be the slowest, but I don't think it would be noticeable in this case).

## Running the Job

## Running Locally

We don't need kubernetes for this job, so you can run from your local virtual environment with:

```
python SQLEmitSample.py
```


## Running on cluster

I didn't test these, but they should work as long as the local works.

If you *do* want to run this inside your cluster, make sure to create the `config.yaml`, generate the job image with the deployment script and apply it. You will need to do the following as well.

### Editing the dependencies
In the `Dockerfile.jinja2`, there is a line:

```
RUN pip install --user .[{{ template.jobdependencies }}]
```

Essentially, when this is installing `edna` inside the job image, we don't want to install all of the edna dependencies, because they can be too big. So we will tell it to install just the dependencies we need. For now, there are 4 possible install modes:

1. `pip install --user .`: Install just the minimal `edna` package
2. `pip install --user .[mysql]`: Install `edna` with `mysql` dependencie to connect to a mysql database
3.`pip install --user .[sklearn]`: Install `edna` plus `scikit-learn`
4.`pip install --user .[full]`: Install the complete `edna` package

You can see the list of possible options in `python/edna/setup.py` in the `recommended` dictionary.

So for a mysql emit job, you will need to add the mysql dependency to the `config.yaml`'s `template.jobdependencies` field. You should be able to figure this out on your own.

### Adding the connection to the server

First you need to create a mysql service inside kubernetes to access the mysql server. Create the `mysql-service.yaml` as below and apply with `kubectl apply -f mysql-service.yaml`:

```
apiVersion: v1
kind: Service
metadata:
  name: mysql-service
spec:
  ports:
    - protocol: TCP
      port: 3306
      targetPort: 3306
```

Then create an endpoint and apply it as well:

```
apiVersion: v1
kind: Endpoints
metadata:
  name: mysql-service
subsets:
  - addresses:
      - ip: <resolv.conf IP here>
    ports:
      - port: 3306
```

Now you can launch an interactive pod (e.g. busybox or something similar), install mysql-client on that pod, and connect to your external database by passing `mysql-service` into the host parameter for `mysql`. Alternatively, you can run a mysql-specific pod as follows:

```
kubectl run -it --rm --image=mysql:5.7 --restart=Never mysql-client -- mysql -h mysql-service -u <username> -p<password>
```

Note -- put the password right after `-p` without spaces. So if your password is `hello`, your command will be:

```
kubectl run -it --rm --image=mysql:5.7 --restart=Never mysql-client -- mysql -h mysql-service -u <username> -phello
```

Technically, you can actually skip the entire service and endpoint creation process and directly use the internal DNS mapped host, with:

```
kubectl run -it --rm --image=mysql:5.7 --restart=Never mysql-client -- mysql -h host.docker.internal -u <username> -phello
```

But this is only a development solution, not a production solution.



For those on Unix only, you should replace the IP in endpoint with the IP from your docker network bridge (see Part 1 Readme for this). If this doesn't work, let me know, and also try to see if stackoverflow has answers (and do some hunting -- if you find the solution, please let me know so I can add it here). If this fails, [try the accepted answer here](https://stackoverflow.com/questions/31324981/how-to-access-host-port-from-docker-container).

If you are on macOS, you should replace it with `docker.for.mac.host.internal`, as per [this stackoverflow question's second answer](https://stackoverflow.com/questions/31324981/how-to-access-host-port-from-docker-container). However, I cannot test this, so please let me know if it doesn't work. You would also probably have to hunt a bit for the solution.

### Running the job

Now you can generate the job and apply it to see it in action. If you have already run it locally, you will notice multiple copies of inserts, since we are primary keying by the actor id, which auto increments.

### SQLUpsert

I have also provided a `SQLUpsertSample` example, where we perform an `upsert` instead of an `insert` operation on the table. Basically, `upsert` updates a row if it already exists in a table. You should be able to run this on your own. the only difference here is the `upsert` fields, which are the fields to update if we find duplicate rows, and the changed Emit primitive from `SQLEmit` to `SQLUpsertEmit`.

# Next steps

At this point, you should have a good grasp on running jobs and applications. 

For those working on the custom resource project, you should have more than enough to get started on your custoom resource schema. This schema will control a Job, so it should, at minimum, contain everything in the `config.yaml` we use to generate the job images.


For those working on analytics project, you also have access to the `edna.process.map.SklearnClassifier` primitive. You can train an Sklearn classifier offline, export its parameters to a file, save that file in your job directory, add the file name to `template.filedependencies` field to copy it into the image, and make sure to pass the file name through `ednaconf.yaml` to the `classifier_path` argument in the primitive.




